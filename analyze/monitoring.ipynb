{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install google-cloud-bigquery matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import warnings\n",
    "\n",
    "PROJECT=\"\" # Enter your Google Project ID\n",
    "LOCATION=\"US\"\n",
    "DATASET=\"cromwell_monitoring\"\n",
    "DATE_RANGE=\"30 DAY\"\n",
    "MAX_GB_PROCESSED=1\n",
    "MAX_ROWS=100000\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*user credentials from Google Cloud SDK.*', module='google.auth')\n",
    "\n",
    "def monitoring_query(dry_run=False):\n",
    "    client = bigquery.Client()\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=dry_run)\n",
    "    query = f\"\"\"\n",
    "        WITH metrics AS (\n",
    "          SELECT\n",
    "            instance_id,\n",
    "            TIMESTAMP_DIFF(MAX(timestamp), MIN(timestamp), SECOND) runtime_duration_sec,\n",
    "            AVG((SELECT AVG(p) FROM UNNEST(cpu_used_percent) p)) cpu_used_percent_avg,\n",
    "            MAX(mem_used_gb) mem_used_gb_max,\n",
    "            [MAX(disk_used_gb[OFFSET(0)]), MAX(disk_used_gb[SAFE_OFFSET(1)])] disk_used_gb_max,\n",
    "            [AVG(disk_read_iops[OFFSET(0)]), AVG(disk_read_iops[SAFE_OFFSET(1)])] disk_read_iops_avg,\n",
    "            [AVG(disk_write_iops[OFFSET(0)]), AVG(disk_write_iops[SAFE_OFFSET(1)])] disk_write_iops_avg\n",
    "          FROM\n",
    "            `{PROJECT}.{DATASET}.metrics`\n",
    "          WHERE\n",
    "            timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {DATE_RANGE})\n",
    "          GROUP BY\n",
    "            instance_id\n",
    "        ),\n",
    "        \n",
    "        results AS (\n",
    "            SELECT\n",
    "              r.project_id, r.zone, r.preemptible,\n",
    "              r.workflow_id, workflow_name, r.task_call_name, r.shard, r.attempt, execution_status,\n",
    "              m.start_time metadata_start_time, TIMESTAMP_DIFF(m.end_time, m.start_time, SECOND) metadata_duration_sec, runtime_duration_sec,\n",
    "              cpu_platform, r.cpu_count cpu_total_cores,\n",
    "              (r.cpu_count * cpu_used_percent_avg / 100) cpu_used_cores_avg,\n",
    "              r.mem_total_gb, mem_used_gb_max,\n",
    "              r.disk_mounts, disk_types, r.disk_total_gb,\n",
    "              (SELECT ARRAY_AGG(x IGNORE NULLS) FROM UNNEST(disk_used_gb_max) x) disk_used_gb_max,\n",
    "              (SELECT ARRAY_AGG(x IGNORE NULLS) FROM UNNEST(disk_read_iops_avg) x) disk_read_iops_avg,\n",
    "              (SELECT ARRAY_AGG(x IGNORE NULLS) FROM UNNEST(disk_write_iops_avg) x) disk_write_iops_avg,\n",
    "              (SELECT SUM(CAST(value AS FLOAT64)) FROM UNNEST(inputs) WHERE type = 'file') inputs_size_gb,\n",
    "              docker_image\n",
    "            FROM\n",
    "              `{PROJECT}.{DATASET}.runtime` r\n",
    "            JOIN\n",
    "              metrics\n",
    "            USING (instance_id)\n",
    "            JOIN\n",
    "              `{PROJECT}.{DATASET}.metadata` m\n",
    "            USING (instance_name)\n",
    "            WHERE\n",
    "              r.start_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {DATE_RANGE})\n",
    "              AND\n",
    "              m.start_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {DATE_RANGE})\n",
    "            ORDER BY\n",
    "              r.start_time DESC\n",
    "        )\n",
    "        \n",
    "        SELECT *\n",
    "        FROM results\n",
    "        WHERE RAND() < {MAX_ROWS}/(SELECT COUNT(*) FROM results)\n",
    "        AND inputs_size_gb IS NOT NULL\n",
    "    \"\"\"\n",
    "    return client.query(\n",
    "        query,\n",
    "        location=LOCATION,\n",
    "        job_config=job_config,\n",
    "    )\n",
    "\n",
    "q = monitoring_query(dry_run=True)\n",
    "gb_processed = q.total_bytes_processed / 1024**3\n",
    "if gb_processed > MAX_GB_PROCESSED:\n",
    "    print(f\"This query will process {gb_processed:.1f} GB when run. Please adjust DATE_RANGE and retry.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    q = monitoring_query()\n",
    "    print(f\"Sample size: {q.result().total_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_SAMPLE_SIZE=5\n",
    "\n",
    "calls = {}\n",
    "for row in q:\n",
    "    if row.task_call_name in calls:\n",
    "        calls[row.task_call_name].append(row)\n",
    "    else:\n",
    "        calls[row.task_call_name] = [row]\n",
    "\n",
    "def plot(i, j, failed, done, metric, total, ylabel, color):\n",
    "    inputs_size_gb_failed = [s.inputs_size_gb for s in failed]\n",
    "    inputs_size_gb_done = [s.inputs_size_gb for s in done]\n",
    "\n",
    "    metric_failed = [s[metric] for s in failed]\n",
    "    metric_done = [s[metric] for s in done]\n",
    "\n",
    "    total_failed = [s[total] for s in failed]\n",
    "    total_done = [s[total] for s in done]\n",
    "\n",
    "    plt.subplot(n, 3, i * 3 + j)\n",
    "\n",
    "    if len(inputs_size_gb_failed) > 0:\n",
    "        plt.plot(inputs_size_gb_failed, metric_failed, 'o', label=f'{metric} (failure)', color='xkcd:orange')\n",
    "        plt.plot(inputs_size_gb_failed, total_failed, 'o', label=f'{total} (failure)', color='xkcd:light orange')\n",
    "\n",
    "    plt.plot(inputs_size_gb_done, metric_done, '.', label=f'{metric} (success)', color=f'xkcd:{color}')\n",
    "    plt.plot(inputs_size_gb_done, total_done, '.', label=f'{total} (success)', color=f'xkcd:light {color}')\n",
    "\n",
    "    plt.xlabel('Inputs (GB)')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "\n",
    "i = 0\n",
    "n = len(calls)\n",
    "fig = plt.figure(figsize=(30, 10 * n))\n",
    "\n",
    "for name, samples in calls.items():\n",
    "    if len(samples) < MIN_SAMPLE_SIZE:\n",
    "        continue\n",
    "\n",
    "    failed = [s for s in samples if s.execution_status == 'Failed']\n",
    "    done = [s for s in samples if s.execution_status == 'Done']\n",
    "\n",
    "    plot(i, 1, failed, done, 'cpu_used_cores_avg', 'cpu_total_cores', 'CPU (cores)', 'green')\n",
    "    plot(i, 2, failed, done, 'mem_used_gb_max', 'mem_total_gb', 'Memory (GB)', 'blue')\n",
    "    plot(i, 3, failed, done, 'disk_used_gb_max', 'disk_total_gb', 'Disk (GB)', 'magenta')\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}